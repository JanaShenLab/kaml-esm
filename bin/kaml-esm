#!/usr/bin/env python3
#
#  ____            ____  __    __                   __       __  __
# |    \          |    \|  \  /  \                 |  \     /  \|  \
# | $$$$  ______   \$$$$| $$ /  $$ ______          | $$\   /  $$| $$
# | $$   /      \   | $$| $$/  $$ |      \  ______ | $$$\ /  $$$| $$
# | $$  |  $$$$$$\  | $$| $$  $$   \$$$$$$\|      \| $$$$\  $$$$| $$
# | $$  | $$  | $$  | $$| $$$$$\  /      $$ \$$$$$$| $$\$$ $$ $$| $$
# | $$_ | $$__/ $$ _| $$| $$ \$$\|  $$$$$$$        | $$ \$$$| $$| $$_____
# | $$ \| $$    $$|   $$| $$  \$$\\$$    $$        | $$  \$ | $$| $$     \
#  \$$$$| $$$$$$$  \$$$$ \$$   \$$ \$$$$$$$         \$$      \$$ \$$$$$$$$
#       | $$
#       | $$
#        \$$
#
##########################################################################
"""
KaML-ESM pipeline with single‐progress‐bar multi-FASTA parallelization.
"""

import argparse
import os
import sys
import re
import subprocess
import urllib.request
import tempfile
import logging
import statistics
import time
import gc
import traceback
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import numpy as np
import torch
import warnings

warnings.filterwarnings(
    "ignore",
    message="Entity ID not found in metadata, using None as default",
    module="esm.utils.structure.protein_complex",
)
warnings.simplefilter("ignore", FutureWarning)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# -------- Global Extraction Parameters --------
ESM2_MODEL = "esm2_t33_650M_UR50D"
ESM2_TOKS_PER_BATCH = 4096
ESM2_TRUNTO = 1022   # switch to ESMC if seq length > 1022
ESM2_INCLUDE = ["per_tok"]
ESM2_REPR = [31]
ESM2_ATTN_ROI = ["LC3D", "ABPP"]
ESM2_NOGPU = True

ESMC_MODEL = "esmc-6b-2024-12"
ESMC_TRUNTO = 2046
ESMC_INCLUDE = ["per_tok"]
ESMC_REPR = [80]
ESMC_ATTN_ROI = ["LC3D", "ABPP"]

# ---- Global Folding Params for Forge Client --------
FORGE_URL = "https://forge.evolutionaryscale.ai"
FORGE_MDL = "esm3-medium-2024-08"

# -------- Global Inference Params --------
norm_dict = {
    0: [2.3, 0.24], 1: [2.29, 0.22], 2: [2.29, 0.23],
    3: [2.31, 0.24], 4: [2.31, 0.24], 5: [2.32, 0.24],
    6: [2.29, 0.26], 7: [2.29, 0.28], 8: [2.32, 0.23],
    9: [2.33, 0.24], 10: [2.28, 0.21], 11: [2.33, 0.25],
    12: [2.32, 0.22], 13: [2.31, 0.22], 14: [2.29, 0.29],
    15: [2.28, 0.20], 16: [2.30, 0.27], 17: [2.31, 0.22],
    18: [2.30, 0.22], 19: [2.28, 0.19],
}
model_pka_dict = {
    "D": 3.7, "E": 4.2, "H": 6.5, "C": 8.5, "K": 10.4, "Y": 10.1,
}
residue_groups = {
    "acidic": ["D", "E", "C", "Y"],
    "basic":  ["H", "K"],
}

THREE_TO_ONE = {
    "ALA": "A", "CYS": "C", "ASP": "D", "GLU": "E", "PHE": "F",
    "GLY": "G", "HIS": "H", "ILE": "I", "LYS": "K", "LEU": "L",
    "MET": "M", "ASN": "N", "PRO": "P", "GLN": "Q", "ARG": "R",
    "SER": "S", "THR": "T", "VAL": "V", "TRP": "W", "TYR": "Y",
}

# For toggling acidic channel if seq > 1022
ACIDIC_MODEL_DIR      = "wts/esm2/acidic"
BASIC_MODEL_DIR       = "wts/esmC/basic"
CBTREE_MODEL_DIR      = "wts/CBtree"
ACIDIC_MODEL_DIR_ESM2 = "wts/esm2/acidic"
ACIDIC_MODEL_DIR_ESMC = "wts/esmC/acidic"

PRELOADED_ACIDIC_WEIGHTS      = {}
PRELOADED_BASIC_WEIGHTS       = {}
PRELOADED_ACIDIC_WEIGHTS_ESM2 = {}
PRELOADED_ACIDIC_WEIGHTS_ESMC = {}

GLOBAL_MLP_CLASSES = {}

# if True, mark sequences as “of concern” to skip safety filter
SKIP_SAFETY = False

########################################
# Basic MLP Classes
########################################
class BasicMLP(torch.nn.Module):
    """MLP for ESMC/basic or any 2560-dim embeddings."""
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(2560, 1024)
        self.bn1 = torch.nn.BatchNorm1d(1024)
        self.fc2 = torch.nn.Linear(1024, 512)
        self.bn2 = torch.nn.BatchNorm1d(512)
        self.fc3 = torch.nn.Linear(512, 64)
        self.bn3 = torch.nn.BatchNorm1d(64)
        self.fc4 = torch.nn.Linear(64, 1)

    def forward(self, x, batch=None):
        act = torch.nn.functional.relu
        drop = torch.nn.Dropout(p=0.2)
        h = act(self.bn1(self.fc1(x))); h = drop(h)
        h = act(self.bn2(self.fc2(h))); h = drop(h)
        h = act(self.bn3(self.fc3(h))); h = drop(h)
        return self.fc4(h)


class AcidicMLP(torch.nn.Module):
    """MLP for ESM2/acidic or any 1280-dim embeddings."""
    def __init__(self):
        super().__init__()
        self.fc1 = torch.nn.Linear(1280, 512)
        self.fc2 = torch.nn.Linear(512, 256)
        self.bn2 = torch.nn.BatchNorm1d(256)
        self.fc3 = torch.nn.Linear(256, 32)
        self.bn3 = torch.nn.BatchNorm1d(32)
        self.fc4 = torch.nn.Linear(32, 1)

    def forward(self, x, batch=None):
        act = torch.nn.functional.relu
        drop = torch.nn.Dropout(p=0.2)
        h = self.fc1(x); h = drop(h)
        h = act(self.bn2(self.fc2(h))); h = drop(h)
        h = act(self.bn3(self.fc3(h))); h = drop(h)
        return self.fc4(h)


########################################
# Setup Logging & Splash
########################################
def setup_logging(log_file, debug=False):
    lvl = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        filename=log_file, filemode="w", level=lvl,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )
    logging.debug("Debug logging enabled.")


def splash():
    print("-" * 80)
    print(r"""
 ____            ____   __    __                   __       __  __
|    \          |    \|  \  /  \                 |  \     /  \|  \
| $$$$  ______   \$$$$| $$ /  $$ ______          | $$\   /  $$| $$
| $$   /      \   | $$| $$/  $$ |      \  ______ | $$$\ /  $$$| $$
| $$  |  $$$$$$\  | $$| $$  $$   \$$$$$$\|      \| $$$$\  $$$$| $$
| $$  | $$  | $$  | $$| $$$$$\  /      $$ \$$$$$$| $$\$$ $$ $$| $$
| $$_ | $$__/ $$ _| $$| $$ \$$\|  $$$$$$$        | $$ \$$$| $$| $$_____
| $$ \| $$    $$|   $$| $$  \$$\\$$    $$        | $$  \$ | $$| $$     \
 \$$$$| $$$$$$$  \$$$$ \$$   \$$ \$$$$$$$         \$$      \$$ \$$$$$$$$
      | $$
      | $$                    Mingzhe Shen & Guy Dayhoff II
       \$$                       March 2025, UMB School of Pharmacy
""")
    print("-" * 80)


def parse_args():
    p = argparse.ArgumentParser(
        description="KaML-ESM pipeline with single‐bar multi-FASTA."
    )
    grp = p.add_mutually_exclusive_group(required=True)
    grp.add_argument("--seq",     type=str, help="Input amino acid sequence")
    grp.add_argument("--pdb",     type=str, help="Path to input PDB file")
    grp.add_argument("--pdbid",   type=str, help="PDB ID to fetch structure")
    grp.add_argument("--uniprot", type=str, help="UniProt ID to fetch seq")
    grp.add_argument("--fasta",   type=str, help="Path to multi-FASTA file")
    p.add_argument("--nopdb",     action="store_true",
                   help="Skip structure output step")
    p.add_argument("--outdir",    type=str, default="output",
                   help="Output directory")
    p.add_argument("--nproc",     type=int, default=1,
                   help="Parallel workers for multi-fasta")
    p.add_argument("--debug",     action="store_true",
                   help="Enable debug logging")
    p.add_argument("--skip_safety", action="store_true",
                   help="Skip safety filter (ESM permission required)")
    p.add_argument("--localfold", action="store_true",
                   help="Use local folding (esm3-open + SCWRL4)")
    p.add_argument("--acidic", type=str, default="esm2",
                   choices=["esm2", "esmC"],
                   help="Acidic channel model (ignored if len>1022)")
    p.add_argument("--basic",  type=str, default="esmC",
                   choices=["esm2", "esmC"],
                   help="Basic channel model")
    p.add_argument("--nocbtree", action="store_true",
                   help="Skip CBTREE predictions")
    return p.parse_args()


########################################
# Additional Helper Functions
########################################
def fetch_uniprot_sequence(uniprot_id):
    url = f"https://www.uniprot.org/uniprot/{uniprot_id}.fasta"
    try:
        with urllib.request.urlopen(url) as resp:
            data = resp.read().decode("utf-8")
        lines = data.splitlines()
        seq = "".join(l.strip() for l in lines if not l.startswith(">"))
        logging.debug("Fetched UniProt %s (len=%d)", uniprot_id, len(seq))
        return seq
    except Exception as e:
        logging.error("Error fetching UniProt %s: %s", uniprot_id, e)
        sys.exit(1)


def fetch_pdb(pdb_id, out_path):
    url = f"https://files.rcsb.org/download/{pdb_id}.pdb"
    try:
        with urllib.request.urlopen(url) as resp:
            pdb_data = resp.read().decode("utf-8")
        with open(out_path, "w") as f:
            f.write(pdb_data)
        logging.debug("Fetched PDB %s -> %s", pdb_id, out_path)
        return out_path
    except Exception as e:
        logging.error("Error fetching PDB %s: %s", pdb_id, e)
        sys.exit(1)


def parse_pdb_to_sequence(pdb_file):
    seq = ""
    info = []
    seen = set()
    idx = 1
    with open(pdb_file) as f:
        for line in f:
            if line.startswith("ATOM") and line[12:16].strip() == "CA":
                res = line[17:20].strip()
                ch  = line[21].strip()
                rn  = line[22:26].strip()
                uid = f"{ch}_{rn}"
                if uid in seen:
                    continue
                seen.add(uid)
                aa = THREE_TO_ONE.get(res.upper(), "X")
                seq += aa
                info.append((res.upper(), rn, ch, idx))
                idx += 1
    logging.debug("Parsed PDB %s: len=%d", pdb_file, len(seq))
    return seq, info


def retry_operation(func, max_retries=3, initial_delay=2,
                    backoff_factor=2, jitter=0.5):
    import random
    delay = initial_delay
    for attempt in range(1, max_retries+1):
        try:
            result = func()
            # verbose unauthorized check
            if "Unauthorized" in str(result):
              print("\n" + "-"*80)
              print("\nAlas, the ESM Forge Token you specified returned an 'unauthorized' error.")
              print("   1. please confirm your token is valid")
              print("   2. update ESM_FORGE_TOKEN environmental variable")
              print("      for example: export ESM_FORGE_TOKEN=<your_esm_forge_token>")
              print("   2. try again\n")
              sys.exit(1)
            # verbose sequence concern check
            elif "potential sequence of concern" in str(result):
              print("\n\n" + "-"*80)
              print("\nAlas, the sequence (protein) was flagged as a potential sequence of concern.")
              print("   1. get permission to skip the safety filter from ESM directly")
              print("   2. once you have permission, use the --skip_safety flag at runtime")
              print("   3. try again\n")
              sys.exit(1)
            # verbose sequence concern check
            if "User does not have permission to skip safety filter" in str(result):
              print("\n\n" + "-"*80)
              print("\nAlas, the forge API says you do not have permission to skip the safety filter.")
              print("   a. get permission to skip the safety filter from ESM directly")
              print("   b. do not use the --skip_safety flag\n")
              sys.exit(1)

            logging.debug("Attempt %d: Success.", attempt)
            return result
        except Exception as e:
            if attempt < max_retries:
                wait = delay + random.uniform(0, jitter)
                logging.debug("Attempt %d/%d fail: %s. Retry in %.2fs...",
                              attempt, max_retries, e, wait)
                time.sleep(wait)
                delay *= backoff_factor
            else:
                logging.debug("Attempt %d/%d fail: %s. No more retries.",
                              attempt, max_retries, e)
                raise


def get_titratable_indices(residue_info, channel):
    idxs = []
    for i, (res, rn, ch, si) in enumerate(residue_info):
        aa = THREE_TO_ONE.get(res.upper(), "X")
        if channel == "acidic" and aa in residue_groups["acidic"]:
            idxs.append(i)
        elif channel == "basic" and aa in residue_groups["basic"]:
            idxs.append(i)
    logging.debug("Titratable %s indices: %s", channel, idxs)
    return idxs


def parse_residues(header, attn_roi):
    patterns = {f: re.compile(rf"{f}=([\d,]+)") for f in attn_roi}
    res = set()
    for f, pat in patterns.items():
        m = pat.search(header)
        if m:
            for n in m.group(1).split(","):
                try:
                    res.add(int(n))
                except ValueError:
                    pass
    parsed = sorted(res)
    logging.debug("Parsed ROI from '%s': %s", header, parsed)
    return parsed


########################################
# ESM2 / ESMC Representation
########################################
def extract_esm2_representation(sequence, header):
    from plmpg.esm2 import pretrained
    from plmpg.esm2.esm.data import FastaBatchedDataset
    from torch.utils.data import DataLoader

    with tempfile.NamedTemporaryFile(
            mode="w+", delete=False, suffix=".fasta"
        ) as tf:
        fasta_header = header if header.startswith(">") else f">{header}"
        tf.write(f"{fasta_header}\n{sequence}\n")
        fasta_path = tf.name

    model, alphabet = pretrained.load_model_and_alphabet(ESM2_MODEL)
    model.eval()
    if torch.cuda.is_available() and not ESM2_NOGPU:
        model = model.cuda()
        logging.debug("ESM2 -> GPU")

    dataset = FastaBatchedDataset.from_file(fasta_path)
    batches = dataset.get_batch_indices(
        ESM2_TOKS_PER_BATCH, extra_toks_per_seq=1
    )
    loader = DataLoader(
        dataset,
        collate_fn=alphabet.get_batch_converter(ESM2_TRUNTO),
        batch_sampler=batches,
    )

    result = {}
    pbar = tqdm(total=1, desc="Extracting from ESM2", ncols=80, ascii=True)
    for labels, strs, toks in loader:
        trunc = min(ESM2_TRUNTO, len(strs[0]))
        with torch.no_grad():
            out = model(
                toks,
                repr_layers=[(i + model.num_layers + 1)
                             % (model.num_layers + 1)
                             for i in ESM2_REPR],
                return_contacts=False,
            )
        reps = {
            layer: t[0, 1:trunc+1].clone().cpu()
            for layer, t in out["representations"].items()
        }
        result = {
            "hdr": labels[0],
            "seq": strs[0],
            "roi": parse_residues(labels[0], ESM2_ATTN_ROI),
        }
        if "per_tok" in ESM2_INCLUDE:
            result["representations"] = reps
        pbar.update(1)
        break
    pbar.close()
    os.remove(fasta_path)
    return result


def extract_esmc_representation(sequence, header):
    from esm.sdk.forge import ESM3ForgeInferenceClient
    from esm.sdk.api import ESMProtein, LogitsConfig

    hdr = header if header.startswith(">") else f">{header}"
    protein = ESMProtein(
        sequence=sequence,
        potential_sequence_of_concern=SKIP_SAFETY,
    )
    token = os.getenv("ESM_FORGE_TOKEN")
    if token is None:
        logging.error("ESM_FORGE_TOKEN not set.")
        sys.exit(1)
    logging.debug("Using ESM_FORGE_TOKEN: %s...", token[:5])

    forge_client = ESM3ForgeInferenceClient(
        model=ESMC_MODEL,
        url=FORGE_URL,
        token=token,
    )
    cfg = {
        "truncate_len": ESMC_TRUNTO,
        "max_retries": 5,
        "initial_delay": 2,
        "backoff_factor": 2,
        "jitter": 0.5,
    }

    def encode_func():
        return forge_client.encode(protein)

    from __main__ import retry_operation
    protein_tensor = retry_operation(
        encode_func, max_retries=cfg["max_retries"]
    )

    result = {
        "hdr": hdr,
        "seq": sequence,
        "roi": parse_residues(hdr, ESMC_ATTN_ROI),
    }
    trunc_length = min(cfg["truncate_len"], len(sequence))
    reps = {}

    for layer_i in tqdm(ESMC_REPR, desc="Extracting from ESMC",
                        ncols=80, ascii=True):
        def extract_layer():
            embed_cfg = LogitsConfig(
                return_hidden_states=True,
                ith_hidden_layer=layer_i,
            )
            out = forge_client.logits(protein_tensor, embed_cfg)
            if out.hidden_states is None:
                raise ValueError("Empty hidden states.")
            return out.hidden_states.squeeze()

        try:
            full_h = retry_operation(
                extract_layer, max_retries=cfg["max_retries"]
            )
            if "per_tok" in ESMC_INCLUDE:
                reps[layer_i] = full_h[1:trunc_length+1].clone().cpu()
        except Exception as e:
            logging.error("ESMC layer %d fail: %s", layer_i, e)
        finally:
            gc.collect()
            time.sleep(1)

    if "per_tok" in ESMC_INCLUDE:
        result["representations"] = reps
    return result


########################################
# Weight Loading & Inference
########################################
def preload_weights(model_dir, channel):
    import torch
    weights = {}
    num_splits = 20
    replicas = 10
    for split in tqdm(
        range(num_splits),
        desc=f"Loading {channel} channel weights",
        ncols=80, ascii=True
    ):
        for rep in range(replicas):
            fname = f"E{split}f{rep}.pth"
            fpath = os.path.join(model_dir, fname)
            if os.path.isfile(fpath):
                state = torch.load(fpath, map_location="cpu")
                if isinstance(state, tuple):
                    state = state[0]
                weights[(split, rep)] = state
            else:
                logging.error("Missing weight: %s", fpath)
    return weights


def inference_worker(ensemble_ids, embeddings, filt_res_info,
                     channel, weight_dict):
    import torch
    import numpy as np
    from __main__ import GLOBAL_MLP_CLASSES, norm_dict, model_pka_dict, THREE_TO_ONE

    worker_preds = {}
    try:
        model = GLOBAL_MLP_CLASSES[channel]()
        if torch.cuda.is_available():
            model = model.cuda()
        model.eval()

        for key in ensemble_ids:
            model.load_state_dict(weight_dict[key])
            if torch.cuda.is_available():
                emb = embeddings.cuda(non_blocking=True)
            else:
                emb = embeddings
            with torch.no_grad():
                outs = model(emb).squeeze(1).cpu().numpy()
            for j, res in enumerate(filt_res_info):
                res_name, res_seq, chain_id, seq_index = res
                uid = f"{res_name}_{seq_index}_{chain_id}"
                raw_pred = outs[j]
                split = key[0]
                std, mean = norm_dict[split]
                pred_back = raw_pred * std + mean
                aa = THREE_TO_ONE.get(res_name.upper(), "X")
                base_pka = model_pka_dict.get(aa, 0.0)
                if abs(raw_pred) < 1e-9:
                    shift = 0.0
                    final = 0.0
                else:
                    shift = pred_back
                    final = shift + base_pka
                worker_preds.setdefault(uid, []).append((final, shift))
        return worker_preds
    except Exception as e:
        logging.error(
            "Inference worker error for channel=%s, ensemble_ids=%s: %s",
            channel, ensemble_ids, e
        )
        logging.error(traceback.format_exc())
        return {}


def run_ensemble_inference(embeddings, filt_res_info, channel,
                           nproc, weight_dict):
    import math
    import numpy as np
    from concurrent.futures import ThreadPoolExecutor, as_completed

    avail_ids = list(weight_dict.keys())
    if nproc < 1:
        nproc = 1
    chunk_size = int(math.ceil(len(avail_ids) / nproc))
    chunks = [
        avail_ids[i : i + chunk_size]
        for i in range(0, len(avail_ids), chunk_size)
    ]

    all_preds = {}
    with ThreadPoolExecutor(max_workers=nproc) as executor:
        futs = [
            executor.submit(
                inference_worker,
                chunk,
                embeddings,
                filt_res_info,
                channel,
                weight_dict,
            )
            for chunk in chunks
        ]
        for fut in tqdm(
            as_completed(futs),
            total=len(futs),
            desc=f"Inference {channel}",
            ncols=80,
            ascii=True,
        ):
            try:
                w_preds = fut.result()
                for uid, tup_list in w_preds.items():
                    all_preds.setdefault(uid, []).extend(tup_list)
            except Exception as e:
                logging.error("Exception in worker thread: %s", e)

    avg_preds = {}
    for uid, tup_list in all_preds.items():
        finals = [t[0] for t in tup_list]
        shifts = [t[1] for t in tup_list]
        avg_final = np.mean(finals) if finals else 0.0
        avg_shift = np.mean(shifts) if shifts else 0.0
        n = len(shifts)
        if n > 1:
            std_err = np.std(shifts, ddof=1) / np.sqrt(n)
        else:
            std_err = 0.0
        avg_preds[uid] = (avg_final, avg_shift, std_err)
    return avg_preds


########################################
# Utility: write_predictions, update_pdb
########################################
def write_predictions_csv(predictions, out_csv):
    import logging
    include_cbtree = any(len(v) == 4 for v in predictions.values())
    if include_cbtree:
        headers = [
            "Residue_ID",
            "Pred_pKa",
            "Pred_Shift",
            "Pred_Err",
            "Conf_pKa",
        ]
        widths = [10, 12, 12, 10, 10]
    else:
        headers = [
            "Residue_ID",
            "Pred_pKa",
            "Pred_Shift",
            "Error",
        ]
        widths = [10, 12, 12, 10]
    header_line = "".join(f"{h:>{w}}" for h, w in zip(headers, widths))
    with open(out_csv, "w") as f:
        f.write(header_line + "\n")
        for uid, vals in predictions.items():
            if len(vals) == 4:
                final, shift, err, cbtree = vals
            elif len(vals) == 3:
                final, shift, err = vals
                cbtree = 0.0
            elif len(vals) == 2:
                final, shift = vals
                err, cbtree = 0.0, 0.0
            else:
                final, shift, err, cbtree = 0.0, 0.0, 0.0, 0.0

            if include_cbtree:
                row = (
                    f"{uid:<{widths[0]}}"
                    f"{final:>{widths[1]}.2f}"
                    f"{shift:>{widths[2]}.2f}"
                    f"{err:>{widths[3]}.2f}"
                    f"{cbtree:>{widths[4]}.2f}"
                )
            else:
                row = (
                    f"{uid:<{widths[0]}}"
                    f"{final:>{widths[1]}.2f}"
                    f"{shift:>{widths[2]}.2f}"
                    f"{err:>{widths[3]}.2f}"
                )
            f.write(row + "\n")
    logging.info("Wrote predictions to %s", out_csv)


def update_pdb_beta_factors(pdb_file, predictions, out_pdb):
    import logging
    updated_lines = []
    residue_counter = 0
    last_key = None
    with open(pdb_file, "r") as f:
        lines = f.readlines()
    for line in lines:
        if line.startswith("ATOM"):
            chain_id = line[21].strip()
            res_seq = line[22:26].strip()
            res_name = line[17:20].strip()
            key = (chain_id, res_seq)
            if key != last_key:
                residue_counter += 1
                last_key = key
            current_uid = f"{res_name}_{residue_counter}_{chain_id}"
            beta_val = predictions.get(current_uid, (0.0, 0.0, 0.0))[1]
            new_beta = f"{beta_val:6.2f}"
            new_line = line[:60] + new_beta + line[66:]
            updated_lines.append(new_line)
        else:
            updated_lines.append(line)
    with open(out_pdb, "w") as f:
        f.writelines(updated_lines)
    logging.info("Updated PDB -> %s", out_pdb)


def shutil_which(executable):
    for path in os.environ.get("PATH", "").split(os.pathsep):
        full = os.path.join(path, executable)
        if os.path.isfile(full) and os.access(full, os.X_OK):
            return full
    return None


########################################
# Folding / SCWRL4 / Forge
########################################
def fold_structure(sequence, out_dir):
    import logging
    import torch
    try:
        from esm.models.esm3 import ESM3
        from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig
        from esm.pretrained import load_local_model
        from esm.utils.constants.models import normalize_model_name
    except ImportError:
        logging.error("ESM3 modules not found.")
        sys.exit(1)
    if len(sequence) > 384:
        logging.warning("Sequence length > 384. ESM3 may truncate tail.")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_name = normalize_model_name("esm3-open")
    model = load_local_model(model_name, device=device)
    if device.type != "cpu":
        model = model.to(torch.bfloat16)
    protein = ESMProtein(sequence=sequence)
    conf = GenerationConfig(track="structure", num_steps=1)
    with tqdm(total=1, desc="Folding structure", ncols=80, ascii=True) as pbar:
        protein = model.generate(protein, conf)
        pbar.update(1)
    folded_pdb = os.path.join(out_dir, "folded_structure.pdb")
    protein.to_pdb(folded_pdb)
    logging.info("Folded structure -> %s", folded_pdb)
    return folded_pdb


def optimize_side_chains(pdb_file, out_dir):
    import logging
    optimized_pdb = os.path.join(out_dir, "optimized_structure.pdb")
    scwrl = shutil_which("Scwrl4")
    if not scwrl:
        logging.error("SCWRL4 not found in PATH.")
        sys.exit(1)
    cmd = [scwrl, "-i", pdb_file, "-o", optimized_pdb]
    logging.info("Running SCWRL4 ...")
    res = subprocess.run(cmd, capture_output=True, text=True)
    if res.returncode != 0:
        logging.error("SCWRL4 fail: %s", res.stderr)
        sys.exit(1)
    logging.info("Optimized -> %s", optimized_pdb)
    return optimized_pdb


def forge_fold_structure(sequence, header, out_dir,
                         max_retries=5, rate_limit=5):
    try:
        from esm.sdk.forge import ESM3ForgeInferenceClient
        from esm.sdk.api import ESMProtein, GenerationConfig
    except ImportError:
        logging.error("ESM Forge modules not found.")
        sys.exit(1)
    token = os.getenv("ESM_FORGE_TOKEN")
    if token is None:
        logging.error("ESM_FORGE_TOKEN not set.")
        sys.exit(1)
    model = ESM3ForgeInferenceClient(
        model=FORGE_MDL, url=FORGE_URL, token=token
    )
    protein = ESMProtein(
        sequence=sequence,
        potential_sequence_of_concern=SKIP_SAFETY,
    )
    retries = 0
    backoff = 1
    from tqdm import tqdm
    import numpy as np
    with tqdm(total=1, desc="Folding structure (Forge)", ncols=80,
              ascii=True, leave=False) as pbar:
        while retries < max_retries:
            try:
                time.sleep(60 / rate_limit)
                conf = GenerationConfig(track="structure", num_steps=1)
                struct = model.generate(protein, conf)
                # verbose unauthorized
                if "Unauthorized" in str(struct):
                  print("\n" + "-"*80)
                  print("\nAlas, the ESM Forge Token you specified returned an 'unauthorized' error.")
                  print("   1. please confirm your token is valid")
                  print("   2. update ESM_FORGE_TOKEN environmental variable")
                  print("      for example: export ESM_FORGE_TOKEN=<your_esm_forge_token>")
                  print("   2. try again\n")
                  sys.exit(1)
                # verbose sequence concern
                if "potential sequence of concern" in str(struct):
                  print("\n\n" + "-"*80)
                  print("\nAlas, the sequence (protein) was flagged as a potential sequence of concern.")
                  print("   1. get permission to skip the safety filter from ESM directly")
                  print("   2. once you have permission, use the --skip_safety flag at runtime")
                  print("   3. try again\n")
                  sys.exit(1)
                if "User does not have permission to skip safety filter" in str(struct):
                  print("\n\n" + "-"*80)
                  print("\nAlas, the forge API says you do not have permission to skip the safety filter.")
                  print("   a. get permission to skip the safety filter from ESM directly")
                  print("   b. do not use the --skip_safety flag\n")
                  sys.exit(1)
                pdb_file = os.path.join(out_dir, "folded_structure.pdb")
                struct.to_pdb(pdb_file)
                logging.info("Folded structure -> %s", pdb_file)
                pbar.update(1)
                return pdb_file
            except Exception as e:
                if "429" in str(e):
                    retries += 1
                    sleep_time = backoff + np.random.uniform(0, 0.5)
                    logging.info("Rate limit exceeded. Retrying... "
                                 "Attempt %d after %.2fs.", retries, sleep_time)
                    time.sleep(sleep_time)
                    backoff *= 2
                    pbar.update(1)
                else:
                    logging.error("Unexpected error during forge fold: %s", e)
                    break
    logging.error("Failed to generate structure via forge.")
    sys.exit(1)


########################################
# CBTREE
########################################
def run_cb_tree_channel(pdb_file, out_dir, unique_id):
    import os
    import pandas as pd
    from pycaret.regression import load_model
    import plmpg.cbtrees.features as features

    _, residue_info = parse_pdb_to_sequence(pdb_file)
    titr_res = ['ASP','GL','CYS','HIS','TYR','LYS']
    titrable_info = [r for r in residue_info if r[0] in titr_res]

    residues_for_cb = [pdb_file] + [[r[2],r[0],r[1]] for r in titrable_info]
    feats = features.generate(residues_for_cb)
    tmp_feats = "features.txt"
    with open(tmp_feats, "w") as f:
        f.write(feats)

    df1 = pd.read_csv(tmp_feats)
    num_cols = [
        'com_min_d0_polar','com_min_d1_polar','com_min_d0_nonpolar','com_min_d1_nonpolar',
        'com_min_d0_bb_NH','com_min_d1_bb_NH','com_min_d0_neg_O','com_min_d1_neg_O',
        'com_min_d0_pos_N','com_min_d1_pos_N','com_min_d0_hbond_o','com_min_d1_hbond_o',
        'com_min_d0_hbond_n','com_min_d1_hbond_n','com_min_d0_hbond_h','com_min_d1_hbond_h',
        'com_min_d0_C_SG','com_min_d1_C_SG','com_n_hv_6','com_n_hv_9','com_n_hv_12','com_n_hv_15',
        'com_npolar_5','com_npolar_10','com_npolar_15','com_polar_5','com_polar_10','com_polar_15',
        'DA1','DA2','DD1','DD2','flexibility','n_sc_C','model_pka','buried_ratio','metal'
    ]
    cat_cols = ['rss','rp2ss','rp4ss','rm2ss','rm4ss','charge','group_code']
    df1 = df1[num_cols + cat_cols]

    model_a = load_model(f"{CBTREE_MODEL_DIR}/acidic",verbose=False)
    model_b = load_model(f"{CBTREE_MODEL_DIR}/basic",verbose=False)

    pred_a = model_a.predict(df1)
    pred_b = model_b.predict(df1)

    # cleanup
    os.system("rm error_record_100.txt 2>/dev/null")
    os.system("rm ridainp 2>/dev/null")
    os.system("rm X:A.dat 2>/dev/null")
    os.system("rm rida_results.dat 2>/dev/null")
    os.system("rm rida_results.tab 2>/dev/null")
    os.system("rm rida_anchor2.tab 2>/dev/null")
    os.system("rm logs.log 2>/dev/null")
    os.system(f"rm {tmp_feats} 2>/dev/null")

    cbtree_dict = {}
    for i, res in enumerate(titrable_info):
        rname, rseq, chain, seq_index = res
        uid = f"{rname}_{seq_index}_{chain}"
        if rname in ["HIS","LYS"]:
            cbtree_dict[uid] = round(pred_b[i],3)
        else:
            cbtree_dict[uid] = round(pred_a[i],3)
    return cbtree_dict


########################################
# Splitting PDB Chains
########################################
def split_pdb_by_chain(pdb_file, out_dir):
    chains = {}
    with open(pdb_file, "r") as f:
        for line in f:
            if line.startswith("ATOM"):
                chain = line[21].strip()
                chains.setdefault(chain, []).append(line)
    chain_files = {}
    for chain, chain_lines in chains.items():
        chain_file = os.path.join(out_dir, f"temp_chain_{chain}.pdb")
        with open(chain_file, "w") as f:
            f.writelines(chain_lines)
        chain_files[chain] = chain_file
    return chain_files


def combine_pdb_files(file_list, out_pdb):
    with open(out_pdb, "w") as outfile:
        for file in file_list:
            with open(file, "r") as infile:
                outfile.write(infile.read())
        outfile.write("END\n")


########################################
# Acidic Toggling
########################################
def get_acidic_embedding(sequence, header, default_acidic):
    seq_len = len(sequence)
    from __main__ import extract_esm2_representation, extract_esmc_representation
    if seq_len <= ESM2_TRUNTO:
        if default_acidic == "esm2":
            out = extract_esm2_representation(sequence, header)
            emb = out["representations"][ESM2_REPR[0]]
            return emb, "acidic_esm2"
        else:
            out = extract_esmc_representation(sequence, header)
            emb = out["representations"][ESMC_REPR[0]]
            return emb, "acidic_esmC"
    else:
        out = extract_esmc_representation(sequence, header)
        emb = out["representations"][ESMC_REPR[0]]
        return emb, "acidic_esmC"


def run_acidic_inference(emb, residue_info, channel_name, nproc):
    if channel_name == "acidic_esm2":
        from __main__ import run_ensemble_inference, PRELOADED_ACIDIC_WEIGHTS_ESM2
        return run_ensemble_inference(
            emb,
            residue_info,
            channel_name,
            nproc,
            PRELOADED_ACIDIC_WEIGHTS_ESM2,
        )
    else:
        from __main__ import run_ensemble_inference, PRELOADED_ACIDIC_WEIGHTS_ESMC
        return run_ensemble_inference(
            emb,
            residue_info,
            channel_name,
            nproc,
            PRELOADED_ACIDIC_WEIGHTS_ESMC,
        )


########################################
# Single Pipeline for one sequence
########################################
def process_single_pipeline(
    sequence,
    header,
    provided_structure_file,
    out_dir,
    args,
    unique_id,
):
    import logging
    from __main__ import (
        split_pdb_by_chain,
        parse_pdb_to_sequence,
        combine_pdb_files,
        get_acidic_embedding,
        run_acidic_inference,
        run_ensemble_inference,
        get_titratable_indices,
        update_pdb_beta_factors,
        write_predictions_csv,
        run_cb_tree_channel,
        fold_structure,
        forge_fold_structure,
        optimize_side_chains,
    )

    if (
        provided_structure_file
        and provided_structure_file.lower().endswith(".pdb")
    ):
        chain_files = split_pdb_by_chain(provided_structure_file, out_dir)
        if len(chain_files) > 1:
            logging.warning(
                "Multiple chains found; model is single-chain but proceeding."
            )
        combined_preds = {}
        updated_chain_files = []

        for chain, chain_file in chain_files.items():
            seq_chain, residue_info = parse_pdb_to_sequence(chain_file)
            chain_header = header + f"_{chain}"

            emb_acidic, acidic_channel = get_acidic_embedding(
                seq_chain, chain_header, args.acidic
            )
            if args.basic == "esm2":
                from __main__ import extract_esm2_representation
                out_b = extract_esm2_representation(
                    seq_chain, chain_header
                )
                emb_basic = out_b["representations"][ESM2_REPR[0]]
            else:
                from __main__ import extract_esmc_representation
                out_b = extract_esmc_representation(
                    seq_chain, chain_header
                )
                emb_basic = out_b["representations"][ESMC_REPR[0]]

            a_idx, a_info = (
                get_titratable_indices(residue_info, "acidic"),
                [
                    residue_info[i]
                    for i in get_titratable_indices(
                        residue_info, "acidic"
                    )
                ],
            )
            b_idx, b_info = (
                get_titratable_indices(residue_info, "basic"),
                [
                    residue_info[i]
                    for i in get_titratable_indices(
                        residue_info, "basic"
                    )
                ],
            )
            emb_a = emb_acidic[a_idx]
            emb_b = emb_basic[b_idx]

            logging.info("Running ensemble: acidic (chain %s)...", chain)
            preds_acidic = run_acidic_inference(
                emb_a, a_info, acidic_channel, args.nproc
            )

            logging.info("Running ensemble: basic (chain %s)...", chain)
            from __main__ import PRELOADED_BASIC_WEIGHTS

            preds_basic = run_ensemble_inference(
                emb_b,
                b_info,
                "basic",
                args.nproc,
                PRELOADED_BASIC_WEIGHTS,
            )

            final_preds = {}
            for res in residue_info:
                rname, rseq, ch_id, seq_index = res
                uid = f"{rname}_{seq_index}_{ch_id}"
                if uid in preds_acidic:
                    final_preds[uid] = preds_acidic[uid]
                elif uid in preds_basic:
                    final_preds[uid] = preds_basic[uid]
                else:
                    final_preds[uid] = (0.0, 0.0, 0.0)

            if not args.nocbtree:
                cbtree = run_cb_tree_channel(
                    chain_file, out_dir, unique_id + f"_{chain}"
                )
                for uid, val in final_preds.items():
                    cbtree_val = cbtree.get(uid, 0.0)
                    final_preds[uid] = (val[0], val[1], val[2], cbtree_val)

            combined_preds.update(final_preds)
            updated_file = chain_file + ".upd.pdb"
            update_pdb_beta_factors(chain_file, final_preds, updated_file)
            updated_chain_files.append(updated_file)

        csv_out = os.path.join(out_dir, "predictions.csv")
        write_predictions_csv(combined_preds, csv_out)
        combined_pdb = os.path.join(out_dir, "predicted_structure.pdb")
        combine_pdb_files(updated_chain_files, combined_pdb)
        logging.info("Pipeline done for %s.", unique_id)
        return

    # Single chain or no PDB
    residue_info = None
    if not provided_structure_file and not args.nopdb:
        if args.localfold:
            folded = fold_structure(sequence, out_dir)
        else:
            folded = forge_fold_structure(
                sequence, header, out_dir
            )
        if not args.nopdb:
            provided_structure_file = (
                optimize_side_chains(folded, out_dir)
                if args.localfold
                else folded
            )

        seq_from_pdb, residue_info = parse_pdb_to_sequence(
            provided_structure_file
        )
        if len(seq_from_pdb) != len(sequence):
            logging.warning("Folded structure mismatch vs input length!")
    if residue_info is None:
        residue_info = [
            (aa, str(i + 1), "X", i + 1)
            for i, aa in enumerate(sequence)
        ]

    emb_acidic, acidic_channel_name = get_acidic_embedding(
        sequence, header, args.acidic
    )

    if args.basic == "esm2":
        from __main__ import extract_esm2_representation

        out_b = extract_esm2_representation(sequence, header)
        emb_basic = out_b["representations"][ESM2_REPR[0]]
    else:
        from __main__ import extract_esmc_representation

        out_b = extract_esmc_representation(sequence, header)
        emb_basic = out_b["representations"][ESMC_REPR[0]]

    a_idx, a_info = (
        get_titratable_indices(residue_info, "acidic"),
        [residue_info[i] for i in get_titratable_indices(residue_info, "acidic")],
    )
    b_idx, b_info = (
        get_titratable_indices(residue_info, "basic"),
        [residue_info[i] for i in get_titratable_indices(residue_info, "basic")],
    )
    emb_a = emb_acidic[a_idx]
    emb_b = emb_basic[b_idx]

    logging.info("Running ensemble: acidic ...")
    preds_acidic = run_acidic_inference(
        emb_a, a_info, acidic_channel_name, args.nproc
    )

    logging.info("Running ensemble: basic ...")
    from __main__ import PRELOADED_BASIC_WEIGHTS

    preds_basic = run_ensemble_inference(
        emb_b, b_info, "basic", args.nproc, PRELOADED_BASIC_WEIGHTS
    )

    final_preds = {}
    for res in residue_info:
        rname, rseq, ch, seq_index = res
        uid = f"{rname}_{seq_index}_{ch}"
        if uid in preds_acidic:
            final_preds[uid] = preds_acidic[uid]
        elif uid in preds_basic:
            final_preds[uid] = preds_basic[uid]
        else:
            final_preds[uid] = (0.0, 0.0, 0.0)

    if not args.nocbtree and provided_structure_file:
        cbtree = run_cb_tree_channel(
            provided_structure_file, out_dir, unique_id
        )
        for uid, val in final_preds.items():
            cbtree_val = cbtree.get(uid, 0.0)
            final_preds[uid] = (val[0], val[1], val[2], cbtree_val)

    csv_out = os.path.join(out_dir, "predictions.csv")
    write_predictions_csv(final_preds, csv_out)

    if not args.nopdb and provided_structure_file:
        pdb_out = os.path.join(out_dir, "predicted_structure.pdb")
        update_pdb_beta_factors(provided_structure_file, final_preds, pdb_out)

    logging.info("Pipeline done for %s.", unique_id)


########################################
# Multi-FASTA Logic, Single Progress Bar
########################################
def process_one_sequence_in_parallel(record, args):
    """
    Runs in each child process (if --nproc>1).
    'record' is (header, sequence). Child logs silenced.
    """
    import sys
    import os

    header, sequence = record
    unique_id = header.split()[0].lstrip(">")

    devnull = open(os.devnull, "w")
    old_stdout, old_stderr = sys.stdout, sys.stderr
    sys.stdout, sys.stderr = devnull, devnull

    try:
        out_subdir = os.path.join(args.outdir, unique_id)
        os.makedirs(out_subdir, exist_ok=True)

        from __main__ import process_single_pipeline

        process_single_pipeline(
            sequence=sequence,
            header=header,
            provided_structure_file=None,
            out_dir=out_subdir,
            args=args,
            unique_id=unique_id,
        )
    finally:
        sys.stdout, sys.stderr = old_stdout, old_stderr
        devnull.close()

    return None


def process_fasta(fasta_file, args):
    """
    Reads multi-FASTA records and spawns separate processes if --nproc>1.
    """
    records = []
    with open(fasta_file, "r") as f:
        header = None
        seq_lines = []
        for line in f:
            line = line.strip()
            if line.startswith(">"):
                if header is not None:
                    records.append((header, "".join(seq_lines)))
                header = line
                seq_lines = []
            else:
                seq_lines.append(line)
        if header is not None:
            records.append((header, "".join(seq_lines)))

    total = len(records)
    if total == 0:
        logging.warning("No sequences found in %s", fasta_file)
        return

    from tqdm import tqdm

    if args.nproc < 2:
        with tqdm(
            total=total,
            desc="FASTA Seqs",
            ascii=True,
            ncols=80,
            position=0,
            dynamic_ncols=False,
            leave=True,
        ) as pbar:
            for rec in records:
                process_one_sequence_in_parallel(rec, args)
                pbar.update(1)
        return

    logging.info("Processing multi-fasta with nproc=%d", args.nproc)
    with ProcessPoolExecutor(max_workers=args.nproc) as executor:
        futs = [
            executor.submit(process_one_sequence_in_parallel, rec, args)
            for rec in records
        ]

        with tqdm(
            total=total,
            desc="FASTA Seqs",
            ascii=True,
            ncols=80,
            position=0,
            dynamic_ncols=False,
            leave=True,
        ) as pbar:
            for _ in as_completed(futs):
                pbar.update(1)


########################################
# main()
########################################
def main():
    args = parse_args()
    global SKIP_SAFETY
    SKIP_SAFETY = args.skip_safety

    splash()
    os.makedirs(args.outdir, exist_ok=True)
    log_file = os.path.join(args.outdir, "pipeline.log")
    setup_logging(log_file, debug=args.debug)
    logging.info("Starting KaML-ESM pipeline...")

    token = os.getenv("ESM_FORGE_TOKEN")
    if token is None or len(token) == 0:
        logging.error("ESM_FORGE_TOKEN not set.")
        print("Alas, you must set the environmental variable ESM_FORGE_TOKEN before running.")
        print("      e.g. export ESM_FORGE_TOKEN=<your_esm_forge_token>\n")
        sys.exit(1)

    global ACIDIC_MODEL_DIR, BASIC_MODEL_DIR, CBTREE_MODEL_DIR
    global ACIDIC_MODEL_DIR_ESM2, ACIDIC_MODEL_DIR_ESMC
    global PRELOADED_ACIDIC_WEIGHTS, PRELOADED_BASIC_WEIGHTS
    global PRELOADED_ACIDIC_WEIGHTS_ESM2, PRELOADED_ACIDIC_WEIGHTS_ESMC
    global GLOBAL_MLP_CLASSES

    # make all wts paths relative to the script location, not cwd
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir   = os.path.abspath(os.path.join(script_dir, os.pardir))
    # override default with user inputs (now relative to repo root)
    ACIDIC_MODEL_DIR      = os.path.join(root_dir, "wts", args.acidic, "acidic")
    BASIC_MODEL_DIR       = os.path.join(root_dir, "wts", args.basic,  "basic")
    ACIDIC_MODEL_DIR_ESM2 = os.path.join(root_dir, "wts", "esm2",   "acidic")
    ACIDIC_MODEL_DIR_ESMC = os.path.join(root_dir, "wts", "esmC",   "acidic")
    CBTREE_MODEL_DIR      = os.path.join(root_dir, "wts", "CBtree")

    GLOBAL_MLP_CLASSES["acidic"]      = AcidicMLP if args.acidic == "esm2" else BasicMLP
    GLOBAL_MLP_CLASSES["basic"]       = AcidicMLP if args.basic  == "esm2" else BasicMLP
    GLOBAL_MLP_CLASSES["acidic_esm2"] = AcidicMLP
    GLOBAL_MLP_CLASSES["acidic_esmC"] = BasicMLP

    from __main__ import preload_weights

    logging.info("Preloading acidic weights from %s", ACIDIC_MODEL_DIR)
    PRELOADED_ACIDIC_WEIGHTS = preload_weights(ACIDIC_MODEL_DIR, "acidic")

    logging.info("Preloading basic weights from %s", BASIC_MODEL_DIR)
    PRELOADED_BASIC_WEIGHTS  = preload_weights(BASIC_MODEL_DIR, "basic")

    logging.info("Preloading acidic ESM2 weights for short sequences ...")
    PRELOADED_ACIDIC_WEIGHTS_ESM2 = preload_weights(ACIDIC_MODEL_DIR_ESM2, "acidic_esm2")

    logging.info("Preloading acidic ESMC weights for long sequences ...")
    PRELOADED_ACIDIC_WEIGHTS_ESMC = preload_weights(ACIDIC_MODEL_DIR_ESMC, "acidic_esmC")

    if args.fasta:
        process_fasta(args.fasta, args)
        logging.info("All multi-FASTA sequences processed. Done.")
        return

    # Single input mode
    sequence = None
    structure_file = None
    header = None

    if args.seq:
        sequence = args.seq
        header = ">user-provided-sequence"
    elif args.uniprot:
        sequence = fetch_uniprot_sequence(args.uniprot)
        header = f">{args.uniprot}"
    elif args.pdb or args.pdbid:
        if args.pdb:
            structure_file = args.pdb
        else:
            structure_file = os.path.join(args.outdir, f"{args.pdbid}.pdb")
            structure_file = fetch_pdb(args.pdbid, structure_file)
        seq_tmp, _ = parse_pdb_to_sequence(structure_file)
        sequence = seq_tmp
        header = f">{structure_file}"
    else:
        logging.error("No valid input provided.")
        sys.exit(1)

    from __main__ import process_single_pipeline

    unique_id = header.split()[0].lstrip(">").replace(":", "_")
    process_single_pipeline(
        sequence, header, structure_file, args.outdir, args, unique_id
    )
    logging.info("Pipeline completed for single input.")
    print("")
    return args


if __name__ == "__main__":
    args = main()
    print("--------------------------------------------------------------------------------")
    print(f"KaML run complete. Outputs written to ./{args.outdir}/\n")

